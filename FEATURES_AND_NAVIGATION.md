# Features, Implementation, and User Navigation

## Feature Overview - What Each Feature Does (Finished, Incomplete, or Not Working)

The Clause application consists of multiple interconnected features, some fully functional, others partially implemented, and several that exist only as UI mockups without backend support. The **Document Upload Feature** is fully functional and allows users to drag-and-drop or browse for PDF files (lease agreements, medical bills, or other legal documents), validates file type (PDF only) and size (10MB maximum), uploads files to the FastAPI backend via FormData POST request to `/upload` endpoint, and automatically triggers PII (Personally Identifiable Information) redaction using spaCy and Presidio Analyzer to detect and redact names, addresses, Social Security numbers, phone numbers, and email addresses, storing encrypted PII mappings separately while saving redacted text for analysis. The **PII Redaction Feature** works by scanning uploaded PDFs, extracting text using PyPDF2, using spaCy NLP models to identify PII entities, redacting sensitive information and replacing it with placeholders, encrypting the original PII data with AES encryption, storing encrypted mappings in JSON files, and providing a summary of redacted items (e.g., "3 names, 2 addresses, 1 SSN redacted") displayed to users after upload. The **Metadata Extraction Feature** is functional and automatically extracts key document details like property address, monthly rent, security deposit amount, lease term dates, landlord and tenant names, and special clauses from uploaded documents using Gemini AI, displays extracted metadata in an editable form on the review page, allows users to confirm or modify extracted information before proceeding to analysis, and stores confirmed metadata for use in the full analysis pipeline. The **Document Analysis Feature** is the core working functionality that processes documents through a multi-stage RAG (Retrieval-Augmented Generation) pipeline: it chunks documents into approximately 4000-token segments with overlap for context preservation, performs vector similarity search against a Snowflake database containing Massachusetts General Laws (Chapters 93A, 186, etc.) with pre-computed Gemini embeddings, retrieves relevant legal sections for each chunk, sends each chunk with legal context to Google's Gemini 2.0 Flash model for violation analysis, identifies illegal clauses (red highlights), risky terms (orange/yellow highlights), and favorable clauses (green highlights), calculates estimated damages for each violation based on Massachusetts law (e.g., triple damages for security deposit violations), generates detailed explanations with legal citations (M.G.L. c. 186 §15B), extracts page coordinates for highlighting using pdfplumber, and consolidates all findings into a comprehensive analysis report stored in JSON format. The **Results Display Feature** is functional and shows analysis results including a summary card with total estimated recovery amount, overall risk assessment (high/medium/low), document metadata, key issues section displaying top 3 violations sorted by priority, financial breakdown showing damages estimate for each finding, interactive PDF viewer with color-coded highlights (red for illegal, orange/yellow for risky, green for favorable), clickable highlights that show violation details when selected, and action buttons for generating demand letters and creating cases. The **PDF Viewer with Highlights Feature** works using react-pdf-highlighter library to render PDFs with interactive annotations, displays color-coded highlights overlaid on the original document at precise page coordinates, allows users to click highlights to see violation details, categories, statutes, explanations, and damages estimates, supports page navigation, zoom controls, and scrolling, and synchronizes highlight selection with the results panel. The **Demand Letter Generation Feature** is partially functional: it generates plain text demand letters using Gemini AI based on analysis results, includes violations with legal citations, damages breakdown, total recovery amount, deadline information, and consequences, but does NOT generate actual PDF files (only plain text), does NOT have a preview popup before download (users see text in a modal), does NOT use LaTeX-to-PDF conversion, does NOT include court-ready formatting, and does NOT support jurisdiction-specific templates. The **Chat/RAG Query Feature** is functional and allows users to ask questions about Massachusetts housing laws or their specific analyzed document, uses the same RAG pipeline to search Snowflake for relevant legal sections, generates answers using Gemini AI with document context if a file_id is provided, displays source citations (relevant M.G.L. sections), supports both general legal questions and document-specific queries, and is accessible through a sidebar chat interface and a dedicated analysis page. The **Voice Chat Feature** exists but has limited functionality: it can record audio using browser MediaRecorder API, transcribes speech to text using ElevenLabs or similar services, detects language (English or Chinese), converts transcribed text to chat queries, but the backend integration may be incomplete and voice responses are not fully implemented. The **Cases Management Feature** is partially functional: it displays a list of uploaded documents from the backend `/documents` endpoint, shows document status (uploaded, processing, completed, failed), displays last activity timestamps, allows users to delete cases, but does NOT have real case management (no case creation workflow, no case lifecycle tracking, no case-to-document relationships, no case status state machine), the cases page essentially shows a document list rather than actual cases, and there's no way to create cases from analysis results (the CreateCaseModal exists but likely doesn't work). The **Notifications Feature** exists only as UI mockups with hardcoded data: the notification bell icon in the header shows mock notifications, the notifications page displays fake notification data, but there's NO backend notification system, NO email notifications (SendGrid not integrated), NO SMS notifications (Twilio not integrated), NO push notifications (Firebase not integrated), NO notification queue, and NO real-time notification delivery. The **Authentication Feature** is completely non-functional: sign-in and sign-up UI pages exist with beautiful forms, password strength indicators, email validation, but there's NO backend authentication, NO JWT tokens, NO user database, NO password hashing, NO session management, NO OAuth integration, forms submit but don't actually authenticate users, and anyone can access any feature without login. The **Dashboard/Analytics Feature** shows mock data only: overview cards display fake numbers (views, profit, products, users), charts show hardcoded data, payment overview uses fake data, and there's NO real analytics tracking, NO user funnel metrics, NO success rate calculations, and NO actual data collection. The **Settings Page** exists with UI for email/SMS notification toggles, privacy settings, but these don't actually work since there's no backend to save preferences. The **Welcome/Landing Page** exists with marketing content, feature cards, call-to-action buttons, but it's basic and doesn't have conversion tracking or lead capture forms. The **Admin Panel** does not exist at all—there's no admin dashboard, no firm management interface, no user monitoring, no analytics, and no platform management tools. The **Firm/Clinic Features** are completely missing: no clinic dashboard, no case routing system, no firm onboarding, no clinic user accounts, no case handoff workflow, and no B2B functionality. The **Payment Processing** is completely absent: no Stripe integration, no PayPal integration, no subscription management, no one-time payments, no success fee collection, and no revenue splitting. The **Evidence Management** has a Solana backend service that can mint NFTs for evidence, but it's not integrated with the main application, there's no UI for uploading evidence, and no association with cases. The **Mobile Optimization** exists with responsive design and breakpoints, but it's not fully optimized for mobile-first experience, and there's no PWA functionality.

## How Everything Currently Works in Frontend and Backend

The application operates through a coordinated frontend-backend architecture where the Next.js frontend handles all user interactions and UI rendering while the FastAPI backend processes business logic, AI analysis, and data storage. When a user navigates to the upload page, the frontend renders a multi-step wizard interface (Choose Type → Upload → Review → Analyze → Results) using React state management to track the current step, file selection, upload progress, and document metadata. The upload process begins when a user selects a PDF file: the frontend validates the file client-side (PDF extension, 10MB size limit), creates a FormData object with the file, displays a progress indicator, and sends a POST request to `http://localhost:8000/upload` (or configured API URL). The FastAPI backend receives the upload request through the `/upload` endpoint in `routes/upload.py`, generates a unique UUID file_id, saves the PDF file to disk in the `app/uploads/` directory, immediately starts PII redaction by calling `redact_pdf()` function from `pii_redaction.py` which uses spaCy to detect entities and Presidio Analyzer to redact them, saves the redacted text to a separate file (`{file_id}_redacted.txt`), encrypts PII mappings using AES encryption and stores them in `data/pii_mappings/`, creates a document entry in `data/documents.json` with file_id, filename, file_path, redacted_text_path, size, uploaded_at timestamp, status ("uploaded"), progress (0), message, and PII redaction summary, and returns the file_id and PII summary to the frontend. The frontend receives the response, stores the file_id in React state and sessionStorage for persistence, displays a success toast with PII protection message, and automatically navigates to the review page (`/upload/review?file_id={file_id}`). On the review page, the frontend immediately calls `/extract-metadata` endpoint with the file_id, which triggers a background task in the backend that uses Gemini AI to extract metadata from the document (property address, rent, deposit, dates, parties), stores extracted metadata in the document JSON entry, updates status to "metadata_extracted", and the frontend polls `/status/{file_id}` every 500ms until status becomes "metadata_extracted", then fetches metadata from `/metadata/{file_id}` endpoint, populates editable form fields with extracted values, and allows users to edit and confirm metadata before proceeding. When the user confirms metadata and clicks "Start Analysis," the frontend sends a POST request to `/confirm-metadata` with the file_id and updated metadata, which triggers the full analysis pipeline. The backend analysis service (`services/analysis_service.py`) runs `run_full_analysis()` as a background task that: loads the redacted text from file (or extracts from PDF if redaction failed), initializes PDFExtractor, DocumentChunker, and RAGAnalyzer classes, chunks the document into ~4000-token segments with overlap, updates progress to 40% with message "Analyzing chunks against MA laws...", for each chunk performs vector similarity search in Snowflake database using `analyzer.search_relevant_laws()` which queries the legal_documents table with vector embeddings to find top 8 most relevant Massachusetts law sections, sends the chunk text along with relevant laws to Gemini 2.0 Flash via `analyzer.analyze_chunk()` which prompts the AI to identify violations, risky terms, and favorable clauses with legal citations, receives analysis results with illegal clauses, risky terms, explanations, and potential recovery amounts, extracts page coordinates using pdfplumber's PDFCoordinateExtractor to find exact positions of highlighted text in the PDF, creates highlight objects with id, pageNumber, color (red/orange/yellow/green), priority, category, text, statute, explanation, damages_estimate, and position coordinates, updates progress incrementally (40% + (chunk_index / total_chunks) \* 40%), consolidates all chunk analyses into a final report, calculates total estimated recovery by summing damages_estimate from all highlights, determines overall risk level (high/medium/low) based on violation severity, creates a complete analysis structure with documentMetadata, analysisSummary, highlights array, keyDetailsDetected, and document_info, saves everything to `data/documents.json` under the file_id key, and updates status to "completed" with progress 100%. Meanwhile, the frontend on the processing/analysis page polls `/status/{file_id}` every 1 second, displays real-time progress updates (0-100%) with contextual messages ("Analyzing chunk 2/5...", "Consolidating findings..."), shows an animated progress bar and spinner, and when status becomes "completed," automatically redirects to `/results/{file_id}`. On the results page, the frontend calls `fetchAnalysis(file_id)` utility function which makes a GET request to `/document/{file_id}`, receives the complete analysis data including highlights, summary, metadata, converts relative PDF URLs to absolute paths for display, handles errors gracefully with fallback to mock data for testing, and populates the results page with analysis summary, estimated recovery amount, key issues cards, financial breakdown, and PDF viewer component. The PDF viewer (`PdfAnalysisViewer.tsx`) loads the PDF using react-pdf library, normalizes highlight positions from the analysis data, renders color-coded highlights overlaid on the PDF at exact coordinates, allows users to click highlights to see details, and synchronizes with the results panel. When a user wants to generate a demand letter, they click the "Generate Demand Letter" button which opens `GenerateLetterModal`, the modal sends a POST request to `/demand-letter/generate` with the complete analysis JSON data, the backend's `generate_demand_letter()` function in `routes/gemini_client.py` builds a comprehensive prompt including violations, damages, sender/recipient info (with defaults if not provided), preferences (tone, deadline), sends the prompt to Gemini 2.0 Flash with optimized token usage (limiting to top 3 highlights), receives plain text demand letter, validates the output, returns letter text to frontend, and the modal displays the letter in a textarea with copy-to-clipboard functionality (but no PDF download). The chat feature works when users type questions in the sidebar chat or analysis page chat interface: the frontend sends POST request to `/chat` with message and optional file_id for document context, the backend's chat endpoint (`routes/chat.py`) loads document context if file_id provided using `format_analysis_context()` to include analysis highlights and findings in the prompt, searches Snowflake for relevant laws using `analyzer.search_relevant_laws()`, generates answer using Gemini with both document context and legal knowledge, returns answer with source citations, and the frontend displays the response in the chat interface with source links. The cases page fetches all documents from `/documents` endpoint, transforms backend document data into case-like objects for display, shows status badges, last activity timestamps, allows deletion via DELETE `/document/{file_id}`, but doesn't actually create or manage cases—it's essentially a document list. All data persistence currently uses JSON file storage (`data/documents.json`) which is loaded into memory on each request, modified, and saved back to disk, creating a bottleneck for concurrent requests and lacking proper database relationships, transactions, or scalability. The frontend uses React hooks (useState, useEffect, useCallback) for state management, Next.js App Router for navigation, react-hot-toast for notifications, and custom API client (`lib/api.ts`) that handles base URL configuration, error handling, retry logic, and TypeScript interfaces for all API responses. The backend uses FastAPI's dependency injection, Pydantic models for request/response validation, background tasks for long-running operations (analysis), CORS middleware for cross-origin requests, and modular route structure for maintainability.

## User Navigation Flow Through the Frontend UI

A user's journey through the application begins at the welcome/landing page (`/welcome`) which displays marketing content, feature highlights, and call-to-action buttons, though users typically navigate directly to the main dashboard or upload flow. From any page, users can access the sidebar navigation (hidden on mobile, toggleable with hamburger menu) which contains links to Dashboard (`/`), Upload (`/upload`), Cases (`/cases`), Analysis (`/analysis`), Results, Notifications (`/notifications`), Settings (`/pages/settings`), Profile (`/profile`), and other pages, along with a chat interface at the bottom of the sidebar for asking legal questions. The main dashboard (`/`) shows overview cards with mock analytics data, an upload zone in the center with drag-and-drop functionality that links to `/upload`, recent activity feed, and AI helper card with quick question buttons. When a user wants to analyze a document, they navigate to `/upload` which presents a multi-step wizard interface. Step 1 (Choose Type) displays three document type options (Lease, Medical Bill, Other) as large clickable cards with icons and descriptions—users select their document type which is stored in state. Step 2 (Upload) shows a drag-and-drop zone with file input, validation messages, and upload button—users either drag a PDF file onto the zone or click to browse, the frontend validates the file (PDF only, 10MB max), shows file name and size, and when they click "Upload Document," the file is sent to the backend, progress bar animates, and upon successful upload with PII redaction summary, they're automatically redirected to Step 3. Step 3 (Review) at `/upload/review?file_id={file_id}` automatically starts metadata extraction when the page loads, shows a loading skeleton while extracting, then displays an editable form with fields for Landlord Name, Tenant Name, Property Address, Lease Term, Monthly Rent, Security Deposit, and Special Clauses—all pre-populated with AI-extracted values that users can edit, with a "Skip & Analyze Now" option to proceed without metadata, and a "Confirm & Start Analysis" button that saves metadata and proceeds. Step 4 (Analyze) shows a processing screen with animated spinner, progress bar (0-100%), real-time status messages ("Analyzing chunk 2/5...", "Consolidating findings..."), estimated time (3-5 minutes), and a cancel button—the page polls the backend every 1 second for status updates, and when analysis completes, automatically redirects to Step 5. Step 5 (Results) at `/results/{file_id}` displays the complete analysis: a hero section with document title and estimated recovery amount in large, prominent text, overall risk assessment badge (High/Medium/Low Risk), key issues section showing top 3 violations as cards with icons, categories, damages estimates, and "View Details" buttons, financial breakdown section listing all findings with damages sorted by amount, interactive PDF viewer on the right side (or below on mobile) showing the original document with color-coded highlights overlaid at exact coordinates—red for illegal clauses, orange/yellow for risky terms, green for favorable clauses—clicking any highlight shows a popup with violation details, statute citations, explanation, and damages estimate, action buttons including "Generate Demand Letter" which opens a modal, "Create Case" button (though this may not be fully functional), and "View Full Analysis" link. The results page also has tabs or sections for viewing all highlights, analysis summary, document metadata, and related actions. When users click "Generate Demand Letter," the `GenerateLetterModal` opens as an overlay, shows a loading state while generating (calls `/demand-letter/generate`), displays the generated plain text letter in a scrollable textarea, provides a "Copy to Clipboard" button, shows letter metadata (total damages, issues count, deadline date), but does NOT provide PDF download or preview popup as intended in the MVP. Users can navigate to `/cases` to see a list of all their uploaded documents displayed as case cards, each showing document title, type icon, status badge (Checking, Ready for action, Failed), last activity timestamp, estimated recovery (if analyzed), and delete button—clicking a case card should navigate to case details, but the case detail page may show mock data. The `/analysis` page provides a dedicated analysis workspace with PDF viewer on the left, chat interface on the right, and ability to ask questions about the document with file_id context. The sidebar chat interface is accessible from any page and allows quick legal questions, with responses appearing in a chat bubble interface. Throughout the application, users see toast notifications (success, error, info) for actions like upload completion, analysis start, errors, etc. The header contains a notification bell icon (showing mock notification count), theme toggle (dark/light mode), user info dropdown (though no actual user data since auth doesn't work), and on mobile, a hamburger menu to toggle the sidebar. Users can access settings, profile, help pages, but most functionality there is non-functional since there's no backend support. The entire UI uses a modern glass-morphism design with gradient backgrounds, smooth animations, responsive breakpoints, and a warm color palette (peach, coral, orchid, gold, mint) that creates a consumer-friendly, approachable legal tech experience, though the underlying functionality is limited to the core document upload and analysis workflow, with most other features existing as UI mockups without backend integration.
